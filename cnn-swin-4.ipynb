{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\nimport tensorflow as tf\nimport tensorflow_addons as tfa\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport pandas as pd\nfrom keras.utils import np_utils\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Activation, Flatten\nfrom keras.layers import Conv2D, MaxPooling2D, BatchNormalization , Input ,concatenate\nfrom keras.losses import categorical_crossentropy,categorical_hinge,hinge,squared_hinge\nfrom keras.models import Model\nfrom keras.regularizers import l2\nfrom keras.callbacks import  EarlyStopping, ModelCheckpoint\nfrom keras.preprocessing.image import ImageDataGenerator ","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-26T09:14:45.944902Z","iopub.execute_input":"2022-05-26T09:14:45.945364Z","iopub.status.idle":"2022-05-26T09:14:50.783219Z","shell.execute_reply.started":"2022-05-26T09:14:45.945278Z","shell.execute_reply":"2022-05-26T09:14:50.782411Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"path='../input/fer2013/fer2013.csv'\ndata=pd.read_csv(path)\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-26T09:14:50.784855Z","iopub.execute_input":"2022-05-26T09:14:50.785500Z","iopub.status.idle":"2022-05-26T09:14:56.139973Z","shell.execute_reply.started":"2022-05-26T09:14:50.785462Z","shell.execute_reply":"2022-05-26T09:14:56.139207Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train=data.loc[data.Usage==\"Training\"]\nTest=data.loc[data.Usage==\"PrivateTest\"]\nval=data.loc[data.Usage==\"PublicTest\"]\ntest=pd.concat([Test,val])","metadata":{"execution":{"iopub.status.busy":"2022-05-26T09:14:56.144863Z","iopub.execute_input":"2022-05-26T09:14:56.146881Z","iopub.status.idle":"2022-05-26T09:14:56.178744Z","shell.execute_reply.started":"2022-05-26T09:14:56.145653Z","shell.execute_reply":"2022-05-26T09:14:56.178068Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train = np.array(list(map(str.split, train.pixels)), np.float32) \nX_test = np.array(list(map(str.split, test.pixels)), np.float32) \nX_train = X_train.reshape(X_train.shape[0], 48, 48, 1) \nX_test = X_test.reshape(X_test.shape[0], 48, 48, 1)","metadata":{"execution":{"iopub.status.busy":"2022-05-26T09:14:56.179910Z","iopub.execute_input":"2022-05-26T09:14:56.180319Z","iopub.status.idle":"2022-05-26T09:15:19.371634Z","shell.execute_reply.started":"2022-05-26T09:14:56.180281Z","shell.execute_reply":"2022-05-26T09:15:19.370766Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train=X_train/255\nX_test=X_test/255\ny_train = train.emotion  \ny_test = test.emotion \ny_train = np_utils.to_categorical(y_train, 7)\ny_test = np_utils.to_categorical(y_test, 7)","metadata":{"execution":{"iopub.status.busy":"2022-05-26T09:15:19.373293Z","iopub.execute_input":"2022-05-26T09:15:19.373942Z","iopub.status.idle":"2022-05-26T09:15:19.452170Z","shell.execute_reply.started":"2022-05-26T09:15:19.373906Z","shell.execute_reply":"2022-05-26T09:15:19.451298Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"x_train shape: {X_train.shape} - y_train shape: {y_train.shape}\")\nprint(f\"x_test shape: {X_test.shape} - y_test shape: {y_test.shape}\")\n\nplt.figure(figsize=(10, 10))\nfor i in range(25):\n    plt.subplot(5, 5, i + 1)\n    \n    plt.xticks([])\n    plt.yticks([])\n    plt.grid(False)\n    plt.imshow(X_train[i])\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-26T09:15:19.453748Z","iopub.execute_input":"2022-05-26T09:15:19.454163Z","iopub.status.idle":"2022-05-26T09:15:20.445042Z","shell.execute_reply.started":"2022-05-26T09:15:19.454122Z","shell.execute_reply":"2022-05-26T09:15:20.444381Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_classes = 7\ninput_shape = (48, 48, 1)\npatch_size = (2, 2)  # 2-by-2 sized patches\ndropout_rate = 0.03  # Dropout rate\nnum_heads = 8  # Attention heads\nembed_dim = 64  # Embedding dimension\nnum_mlp = 256  # MLP layer size\nqkv_bias = True  # Convert embedded patches to query, key, and values with a learnable additive value\nwindow_size = 2  # Size of attention window\nshift_size = 1  # Size of shifting window\nimage_dimension = 48  # Initial image size\n\nnum_patch_x = input_shape[0] // patch_size[0]\nnum_patch_y = input_shape[1] // patch_size[1]\n\nlearning_rate = 1e-3\nbatch_size = 128\nnum_epochs = 100\nvalidation_split = 0.1\nweight_decay = 0.0001\nlabel_smoothing = 0.1","metadata":{"execution":{"iopub.status.busy":"2022-05-26T09:15:20.445979Z","iopub.execute_input":"2022-05-26T09:15:20.446383Z","iopub.status.idle":"2022-05-26T09:15:20.454528Z","shell.execute_reply.started":"2022-05-26T09:15:20.446332Z","shell.execute_reply":"2022-05-26T09:15:20.453653Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def window_partition(x, window_size):\n    _, height, width, channels = x.shape\n    patch_num_y = height // window_size\n    patch_num_x = width // window_size\n    x = tf.reshape(\n        x, shape=(-1, patch_num_y, window_size, patch_num_x, window_size, channels)\n    )\n    x = tf.transpose(x, (0, 1, 3, 2, 4, 5))\n    windows = tf.reshape(x, shape=(-1, window_size, window_size, channels))\n    return windows\n\n\ndef window_reverse(windows, window_size, height, width, channels):\n    patch_num_y = height // window_size\n    patch_num_x = width // window_size\n    x = tf.reshape(\n        windows,\n        shape=(-1, patch_num_y, patch_num_x, window_size, window_size, channels),\n    )\n    x = tf.transpose(x, perm=(0, 1, 3, 2, 4, 5))\n    x = tf.reshape(x, shape=(-1, height, width, channels))\n    return x\n\n\nclass DropPath(layers.Layer):\n    def __init__(self, drop_prob=None, **kwargs):\n        super(DropPath, self).__init__(**kwargs)\n        self.drop_prob = drop_prob\n\n    def call(self, x):\n        input_shape = tf.shape(x)\n        batch_size = input_shape[0]\n        rank = x.shape.rank\n        shape = (batch_size,) + (1,) * (rank - 1)\n        random_tensor = (1 - self.drop_prob) + tf.random.uniform(shape, dtype=x.dtype)\n        path_mask = tf.floor(random_tensor)\n        output = tf.math.divide(x, 1 - self.drop_prob) * path_mask\n        return output","metadata":{"execution":{"iopub.status.busy":"2022-05-26T09:15:20.456134Z","iopub.execute_input":"2022-05-26T09:15:20.456775Z","iopub.status.idle":"2022-05-26T09:15:20.473207Z","shell.execute_reply.started":"2022-05-26T09:15:20.456732Z","shell.execute_reply":"2022-05-26T09:15:20.472420Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class WindowAttention(layers.Layer):\n    def __init__(\n        self, dim, window_size, num_heads, qkv_bias=True, dropout_rate=0.0, **kwargs\n    ):\n        super(WindowAttention, self).__init__(**kwargs)\n        self.dim = dim\n        self.window_size = window_size\n        self.num_heads = num_heads\n        self.scale = (dim // num_heads) ** -0.5\n        self.qkv = layers.Dense(dim * 3, use_bias=qkv_bias)\n        self.dropout = layers.Dropout(dropout_rate)\n        self.proj = layers.Dense(dim)\n\n    def build(self, input_shape):\n        num_window_elements = (2 * self.window_size[0] - 1) * (\n            2 * self.window_size[1] - 1\n        )\n        self.relative_position_bias_table = self.add_weight(\n            shape=(num_window_elements, self.num_heads),\n            initializer=tf.initializers.Zeros(),\n            trainable=True,\n        )\n        coords_h = np.arange(self.window_size[0])\n        coords_w = np.arange(self.window_size[1])\n        coords_matrix = np.meshgrid(coords_h, coords_w, indexing=\"ij\")\n        coords = np.stack(coords_matrix)\n        coords_flatten = coords.reshape(2, -1)\n        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]\n        relative_coords = relative_coords.transpose([1, 2, 0])\n        relative_coords[:, :, 0] += self.window_size[0] - 1\n        relative_coords[:, :, 1] += self.window_size[1] - 1\n        relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1\n        relative_position_index = relative_coords.sum(-1)\n\n        self.relative_position_index = tf.Variable(\n            initial_value=tf.convert_to_tensor(relative_position_index), trainable=False\n        )\n\n    def call(self, x, mask=None):\n        _, size, channels = x.shape\n        head_dim = channels // self.num_heads\n        x_qkv = self.qkv(x)\n        x_qkv = tf.reshape(x_qkv, shape=(-1, size, 3, self.num_heads, head_dim))\n        x_qkv = tf.transpose(x_qkv, perm=(2, 0, 3, 1, 4))\n        q, k, v = x_qkv[0], x_qkv[1], x_qkv[2]\n        q = q * self.scale\n        k = tf.transpose(k, perm=(0, 1, 3, 2))\n        attn = q @ k\n\n        num_window_elements = self.window_size[0] * self.window_size[1]\n        relative_position_index_flat = tf.reshape(\n            self.relative_position_index, shape=(-1,)\n        )\n        relative_position_bias = tf.gather(\n            self.relative_position_bias_table, relative_position_index_flat\n        )\n        relative_position_bias = tf.reshape(\n            relative_position_bias, shape=(num_window_elements, num_window_elements, -1)\n        )\n        relative_position_bias = tf.transpose(relative_position_bias, perm=(2, 0, 1))\n        attn = attn + tf.expand_dims(relative_position_bias, axis=0)\n\n        if mask is not None:\n            nW = mask.get_shape()[0]\n            mask_float = tf.cast(\n                tf.expand_dims(tf.expand_dims(mask, axis=1), axis=0), tf.float32\n            )\n            attn = (\n                tf.reshape(attn, shape=(-1, nW, self.num_heads, size, size))\n                + mask_float\n            )\n            attn = tf.reshape(attn, shape=(-1, self.num_heads, size, size))\n            attn = keras.activations.softmax(attn, axis=-1)\n        else:\n            attn = keras.activations.softmax(attn, axis=-1)\n        attn = self.dropout(attn)\n\n        x_qkv = attn @ v\n        x_qkv = tf.transpose(x_qkv, perm=(0, 2, 1, 3))\n        x_qkv = tf.reshape(x_qkv, shape=(-1, size, channels))\n        x_qkv = self.proj(x_qkv)\n        x_qkv = self.dropout(x_qkv)\n        return x_qkv","metadata":{"execution":{"iopub.status.busy":"2022-05-26T09:15:20.477314Z","iopub.execute_input":"2022-05-26T09:15:20.477989Z","iopub.status.idle":"2022-05-26T09:15:20.501007Z","shell.execute_reply.started":"2022-05-26T09:15:20.477952Z","shell.execute_reply":"2022-05-26T09:15:20.500279Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class SwinTransformer(layers.Layer):\n    def __init__(\n        self,\n        dim,\n        num_patch,\n        num_heads,\n        window_size=7,\n        shift_size=0,\n        num_mlp=1024,\n        qkv_bias=True,\n        dropout_rate=0.0,\n        **kwargs,\n    ):\n        super(SwinTransformer, self).__init__(**kwargs)\n\n        self.dim = dim  # number of input dimensions\n        self.num_patch = num_patch  # number of embedded patches\n        self.num_heads = num_heads  # number of attention heads\n        self.window_size = window_size  # size of window\n        self.shift_size = shift_size  # size of window shift\n        self.num_mlp = num_mlp  # number of MLP nodes\n\n        self.norm1 = layers.LayerNormalization(epsilon=1e-5)\n        self.attn = WindowAttention(\n            dim,\n            window_size=(self.window_size, self.window_size),\n            num_heads=num_heads,\n            qkv_bias=qkv_bias,\n            dropout_rate=dropout_rate,\n        )\n        self.drop_path = DropPath(dropout_rate)\n        self.norm2 = layers.LayerNormalization(epsilon=1e-5)\n\n        self.mlp = keras.Sequential(\n            [\n                \n                layers.Dense(num_mlp),\n                layers.Activation(keras.activations.gelu),\n                layers.Dropout(dropout_rate),\n                layers.Dense(dim),\n                layers.Dropout(dropout_rate),\n            ]\n        )\n\n        if min(self.num_patch) < self.window_size:\n            self.shift_size = 0\n            self.window_size = min(self.num_patch)\n\n    def build(self, input_shape):\n        if self.shift_size == 0:\n            self.attn_mask = None\n        else:\n            height, width = self.num_patch\n            h_slices = (\n                slice(0, -self.window_size),\n                slice(-self.window_size, -self.shift_size),\n                slice(-self.shift_size, None),\n            )\n            w_slices = (\n                slice(0, -self.window_size),\n                slice(-self.window_size, -self.shift_size),\n                slice(-self.shift_size, None),\n            )\n            mask_array = np.zeros((1, height, width, 1))\n            count = 0\n            for h in h_slices:\n                for w in w_slices:\n                    mask_array[:, h, w, :] = count\n                    count += 1\n            mask_array = tf.convert_to_tensor(mask_array)\n\n            # mask array to windows\n            mask_windows = window_partition(mask_array, self.window_size)\n            mask_windows = tf.reshape(\n                mask_windows, shape=[-1, self.window_size * self.window_size]\n            )\n            attn_mask = tf.expand_dims(mask_windows, axis=1) - tf.expand_dims(\n                mask_windows, axis=2\n            )\n            attn_mask = tf.where(attn_mask != 0, -100.0, attn_mask)\n            attn_mask = tf.where(attn_mask == 0, 0.0, attn_mask)\n            self.attn_mask = tf.Variable(initial_value=attn_mask, trainable=False)\n\n    def call(self, x):\n        height, width = self.num_patch\n        _, num_patches_before, channels = x.shape\n        x_skip = x\n        x = self.norm1(x)\n        x = tf.reshape(x, shape=(-1, height, width, channels))\n        if self.shift_size > 0:\n            shifted_x = tf.roll(\n                x, shift=[-self.shift_size, -self.shift_size], axis=[1, 2]\n            )\n        else:\n            shifted_x = x\n\n        x_windows = window_partition(shifted_x, self.window_size)\n        x_windows = tf.reshape(\n            x_windows, shape=(-1, self.window_size * self.window_size, channels)\n        )\n        attn_windows = self.attn(x_windows, mask=self.attn_mask)\n\n        attn_windows = tf.reshape(\n            attn_windows, shape=(-1, self.window_size, self.window_size, channels)\n        )\n        shifted_x = window_reverse(\n            attn_windows, self.window_size, height, width, channels\n        )\n        if self.shift_size > 0:\n            x = tf.roll(\n                shifted_x, shift=[self.shift_size, self.shift_size], axis=[1, 2]\n            )\n        else:\n            x = shifted_x\n\n        x = tf.reshape(x, shape=(-1, height * width, channels))\n        x = self.drop_path(x)\n        x = x_skip + x\n        x_skip = x\n        x = self.norm2(x)\n        x = self.mlp(x)\n        x = self.drop_path(x)\n        x = x_skip + x\n        return x","metadata":{"execution":{"iopub.status.busy":"2022-05-26T09:15:20.508387Z","iopub.execute_input":"2022-05-26T09:15:20.508719Z","iopub.status.idle":"2022-05-26T09:15:20.534887Z","shell.execute_reply.started":"2022-05-26T09:15:20.508683Z","shell.execute_reply":"2022-05-26T09:15:20.534166Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class PatchExtract(layers.Layer):\n    def __init__(self, patch_size, **kwargs):\n        super(PatchExtract, self).__init__(**kwargs)\n        self.patch_size_x = patch_size[0]\n        self.patch_size_y = patch_size[0]\n\n    def call(self, images):\n        batch_size = tf.shape(images)[0]\n        patches = tf.image.extract_patches(\n            images=images,\n            sizes=(1, self.patch_size_x, self.patch_size_y, 1),\n            strides=(1, self.patch_size_x, self.patch_size_y, 1),\n            rates=(1, 1, 1, 1),\n            padding=\"VALID\",\n        )\n        patch_dim = patches.shape[-1]\n        patch_num = patches.shape[1]\n        return tf.reshape(patches, (batch_size, patch_num * patch_num, patch_dim))\n\n\nclass PatchEmbedding(layers.Layer):\n    def __init__(self, num_patch, embed_dim, **kwargs):\n        super(PatchEmbedding, self).__init__(**kwargs)\n        self.num_patch = num_patch\n        self.proj = layers.Dense(embed_dim)\n        self.pos_embed = layers.Embedding(input_dim=num_patch, output_dim=embed_dim)\n\n    def call(self, patch):\n        pos = tf.range(start=0, limit=self.num_patch, delta=1)\n        return self.proj(patch) + self.pos_embed(pos)\n\n\nclass PatchMerging(tf.keras.layers.Layer):\n    def __init__(self, num_patch, embed_dim):\n        super(PatchMerging, self).__init__()\n        self.num_patch = num_patch\n        self.embed_dim = embed_dim\n        self.linear_trans = layers.Dense(2 * embed_dim, use_bias=False)\n\n    def call(self, x):\n        height, width = self.num_patch\n        _, _, C = x.get_shape().as_list()\n        x = tf.reshape(x, shape=(-1, height, width, C))\n        x0 = x[:, 0::2, 0::2, :]\n        x1 = x[:, 1::2, 0::2, :]\n        x2 = x[:, 0::2, 1::2, :]\n        x3 = x[:, 1::2, 1::2, :]\n        x = tf.concat((x0, x1, x2, x3), axis=-1)\n        x = tf.reshape(x, shape=(-1, (height // 2) * (width // 2), 4 * C))\n        return self.linear_trans(x)","metadata":{"execution":{"iopub.status.busy":"2022-05-26T09:15:20.540961Z","iopub.execute_input":"2022-05-26T09:15:20.541568Z","iopub.status.idle":"2022-05-26T09:15:20.557890Z","shell.execute_reply.started":"2022-05-26T09:15:20.541531Z","shell.execute_reply":"2022-05-26T09:15:20.557150Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def CNN_Layer(input_shape):\n\n    model = tf.keras.Sequential()\n  \n    model.add(Conv2D(32, kernel_size=(3,3), activation='relu', input_shape=(48, 48,1)))\n    model.add(Conv2D(32,kernel_size=(3,3),activation='relu',padding='same'))\n    model.add(BatchNormalization())\n    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2),padding='same'))\n    model.add(Dropout(0.1))\n\n    model.add(Conv2D(64, kernel_size=(3, 3), activation='relu', padding='same'))\n    model.add(Conv2D(64, kernel_size=(3, 3), activation='relu', padding='same'))\n    model.add(BatchNormalization())\n    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2),padding='same'))\n    model.add(Dropout(0.1))\n\n    model.add(Conv2D(128, kernel_size=(3, 3), activation='relu', padding='same'))\n    model.add(Conv2D(128, kernel_size=(3, 3), activation='relu', padding='same'))\n    model.add(BatchNormalization())\n    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2),padding='same'))\n    model.add(Dropout(0.5))\n    model.add(tf.keras.layers.Flatten())\n    model.add(tf.keras.layers.Dense(1024, activation='relu'))\n    return model","metadata":{"execution":{"iopub.status.busy":"2022-05-26T09:15:20.559197Z","iopub.execute_input":"2022-05-26T09:15:20.559647Z","iopub.status.idle":"2022-05-26T09:15:20.573060Z","shell.execute_reply.started":"2022-05-26T09:15:20.559612Z","shell.execute_reply":"2022-05-26T09:15:20.572146Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"    input = layers.Input(input_shape)\n    x = layers.RandomCrop(image_dimension, image_dimension)(input)\n    x = layers.RandomFlip(\"horizontal\")(x)\n    x = PatchExtract(patch_size)(x)\n    x = PatchEmbedding(num_patch_x * num_patch_y, embed_dim)(x)\n    x = SwinTransformer(\n        dim=embed_dim,\n        num_patch=(num_patch_x, num_patch_y),\n        num_heads=num_heads,\n        window_size=window_size,\n        shift_size=0,\n        num_mlp=num_mlp,\n        qkv_bias=qkv_bias,\n        dropout_rate=dropout_rate,\n        )(x)\n    x = SwinTransformer(\n        dim=embed_dim,\n        num_patch=(num_patch_x, num_patch_y),\n        num_heads=num_heads,\n        window_size=window_size,\n        shift_size=0,\n        num_mlp=num_mlp,\n        qkv_bias=qkv_bias,\n        dropout_rate=dropout_rate,\n        )(x)\n    x = SwinTransformer(\n        dim=embed_dim,\n        num_patch=(num_patch_x, num_patch_y),\n        num_heads=num_heads,\n        window_size=window_size,\n        shift_size=0,\n        num_mlp=num_mlp,\n        qkv_bias=qkv_bias,\n        dropout_rate=dropout_rate,\n        )(x)\n    x = SwinTransformer(\n        dim=embed_dim,\n        num_patch=(num_patch_x, num_patch_y),\n        num_heads=num_heads,\n        window_size=window_size,\n        shift_size=shift_size,\n        num_mlp=num_mlp,\n        qkv_bias=qkv_bias,\n        dropout_rate=dropout_rate,\n    )(x)\n    x = PatchMerging((num_patch_x, num_patch_y), embed_dim=embed_dim)(x)\n    x = layers.GlobalAveragePooling1D()(x)\n    x =layers.Dense(1024, activation='relu')(x)###\n    \n    #output = layers.Dense(num_classes, activation=\"softmax\")(x)","metadata":{"execution":{"iopub.status.busy":"2022-05-26T09:15:20.574520Z","iopub.execute_input":"2022-05-26T09:15:20.574878Z","iopub.status.idle":"2022-05-26T09:15:24.762335Z","shell.execute_reply.started":"2022-05-26T09:15:20.574844Z","shell.execute_reply":"2022-05-26T09:15:24.761529Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model1=CNN_Layer(input_shape)\n#MergeModel = concatenate([model1.output, x])\n#MergeModel=tf.keras.layers.Dot(axes=1)([model1.output, x])\n#MergeModel=tf.keras.layers.Maximum()([model1.output, x])\n#MergeModel=tf.keras.layers.Multiply()([model1.output, x])\nMergeModel=tf.keras.layers.Average()([model1.output, x])\nm = Dense(1024, activation='relu')(MergeModel)\nm = Dense(7, activation='softmax')(m)\nmodel = Model(inputs=[model1.input,input], outputs=m)","metadata":{"execution":{"iopub.status.busy":"2022-05-26T09:15:24.763496Z","iopub.execute_input":"2022-05-26T09:15:24.763871Z","iopub.status.idle":"2022-05-26T09:15:24.898440Z","shell.execute_reply.started":"2022-05-26T09:15:24.763834Z","shell.execute_reply":"2022-05-26T09:15:24.897499Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"execution":{"iopub.status.busy":"2022-05-26T09:15:24.899808Z","iopub.execute_input":"2022-05-26T09:15:24.900181Z","iopub.status.idle":"2022-05-26T09:15:24.915057Z","shell.execute_reply.started":"2022-05-26T09:15:24.900136Z","shell.execute_reply":"2022-05-26T09:15:24.910853Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import KFold\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n# Define per-fold score containers\nacc_per_fold = []\nloss_per_fold = []\n# Merge inputs and targets\ninputs = np.concatenate((X_train, X_test), axis=0)\ntargets = np.concatenate((y_train, y_test), axis=0)\n# Define the K-fold Cross Validator\nkfold = KFold(n_splits=10, shuffle=True)\n\n# K-fold Cross Validation model evaluation\nfold_no = 1\nfor train, test in kfold.split(inputs, targets):\n    model.compile(\n    loss=keras.losses.CategoricalCrossentropy(label_smoothing=label_smoothing),\n    optimizer=tfa.optimizers.AdamW(\n        learning_rate=learning_rate, weight_decay=weight_decay\n    ),\n    metrics=[\n        keras.metrics.CategoricalAccuracy(name=\"accuracy\"),\n        keras.metrics.TopKCategoricalAccuracy(5, name=\"top-5-accuracy\"),\n    ],\n)\n# Generate a print\n    print('------------------------------------------------------------------------')\n    print(f'Training for fold {fold_no} ...')\n\n  # Fit data to model\n    history = model.fit([inputs[train],inputs[train]], targets[train],\n              batch_size=batch_size,\n              epochs=20,\n              verbose=1)\n    plt.plot(history.history[\"accuracy\"], label=\"train_accuracy\")\n    #plt.plot(history.history[\"val_accuracy\"], label=\"val_accuracy\")\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(\"accuracy\")\n    plt.title(\"Train and Validation accuracy Over Epochs\", fontsize=14)\n    plt.legend()\n    plt.grid()\n    plt.show()\n    \n\n  # Generate generalization metrics\n    scores = model.evaluate([inputs[test],inputs[test]], targets[test], verbose=0)\n    print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n    acc_per_fold.append(scores[1] * 100)\n    loss_per_fold.append(scores[0])\n    results = model.predict([inputs[test],inputs[test]], batch_size=128)\n    predicted_classes = np.argmax(results, axis=1)\n    y_classes=np.argmax(targets[test], axis=1)\n    cm = confusion_matrix(y_classes,predicted_classes,normalize='true')\n    disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n    disp.plot()\n    plt.show()\n\n  # Increase fold number\n    fold_no = fold_no + 1\n\n# == Provide average scores ==\nprint('------------------------------------------------------------------------')\nprint('Score per fold')\nfor i in range(0, len(acc_per_fold)):\n    print('------------------------------------------------------------------------')\n    print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n    print('------------------------------------------------------------------------')\n    print('Average scores for all folds:')\n    print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n    print(f'> Loss: {np.mean(loss_per_fold)}')\n    print('------------------------------------------------------------------------')","metadata":{"execution":{"iopub.status.busy":"2022-05-26T09:15:24.916824Z","iopub.execute_input":"2022-05-26T09:15:24.917526Z"},"trusted":true},"execution_count":null,"outputs":[]}]}